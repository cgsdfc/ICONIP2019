\begin{abstract}
    Evaluating generative dialogue systems with automatic metrics is a difficult problem. It has been shown that the word overlap metrics such as BLEU and the word embedding metrics correlate poorly with human judgments. In this paper, we highlight yet another problem of evaluation with metrics through an empirical study. We show that regardless of the correlation with human judgments, the scores of these metrics do not correlate well with one another, which means fine-tuning the system on one set of metrics may yield a gotcha when testing with another set of metrics. This infeasibility again adds to the ineffectiveness of evaluation done alone with metrics. To the end of utilizing metrics at its best, we offer several pragmatic suggestions on the use of automatic metrics to avoid the issue of metric inconsistency.
    \keywords{Automatic Metrics, Response Generation, Chatbot}
\end{abstract}