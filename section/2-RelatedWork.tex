\section{Related Work}
Previous works on automatic metrics have mostly focused on the correlation between the metrics and human judgments. After all, the advantages of automatic metrics would be largely compromised if they cannot reflect human judgments. For example, in machine translation, where automatic metrics have been established to reduce human labor, almost every newly proposed metrics report an improved human correlation, as in \cite{NIST}, \cite{METEOR}, and \cite{chrf}.

In dialogue response generation, to address the issue of low metric-human correlation, various semantic-based methods have been proposed. For example, the ADEM metric proposed by Lowe et al. models the human judgments with a feed-forward neural network \cite{ADEM}. Their concise architecture achieves a high correlation with human scores in various settings. The RUBER metric proposed by Tao et al. takes an asymmetric approach w.r.t the response-context pair and response-reference pair, where the former is modeled by a neural network and the latter is measured by an embedding-based metric. They combined two scores with various heuristics and achieved improvement on a Chinese corpus \cite{RUBER}. In brief, correlation with human judgments has always been the supervised signal guiding the evolution of automatic metrics.

On the other hand, automatic metrics have been constantly doubted of their capability to reflect human judgments. In one of the earliest attempts to the response generation problem, Ritter et al. made an initial examination on the suitability of BLEU to this field leveraging the human data they collected \cite{Ritter11}. They found that the BLEU scores were very low even on the system level and the correlation was modest. Shang et al. argued that BLEU did not apply as the reasonable responses are too vast for the references to cover \cite{Shang}.

Another popular metric is perplexity, which is generally used to evaluate statistical language models. Vinyals et al. showed that their Seq2Seq dialogue model achieved a much lower perplexity than the n-gram baseline, but they also admitted the drawbacks of using such a metric \cite{GoogleChatbot}. Serban et al. also used perplexity to evaluate their models in \cite{HRED}, along with other metrics. Like in \cite{GoogleChatbot}, they were not clear about how well these metrics accounted for the grammatical correctness and semantic coherence of the responses.

An extensive study of metric-human correlation was conducted by Liu et al. \cite{HowNot}. Their study took into account of various settings, such as both retrieval and generative models and both technical and non-technical datasets. It revealed that although these metrics can distinguish state-of-art models from baseline models, none of them correlate highly with human scores. Their work has left us two questions towards better automatic metrics:
\begin{enumerate}
    \item How can we improve the metric-human correlation?
    \item Why do the existing metrics correlate badly with human judgments?
\end{enumerate}
Previous works that proposed enhanced metrics endeavored to answer the first question, while we try to shed some light on the second one. In particular, we are curious about what can we learn when the scores from different metrics are put together and compared across various settings.

To understand why some metrics correlate poorly with human judgments, we show that there are general inconsistencies among these metrics. Intuitively, if a set of metrics have low pairwise correlations, it is less likely that all of them would correlate highly with human judgments, which indicates that some of them must correlate poorly with human judgments. Along this line of thought, we arrive at an intermediate explanation to the poor performance of automatic metrics, backed by an empirical study. With this in mind, we can partially understand why these metrics fail and make improvements from the lesson we learned.
