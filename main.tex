\documentclass[runningheads]{llncs}

\usepackage{caption} % change the font size of figure captions.
\captionsetup[figure]{font=small}
\usepackage{graphicx}
\usepackage{array}
\usepackage{booktabs}
\usepackage{float}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{float}
\usepackage{subcaption}
\usepackage{color}

\renewcommand\UrlFont{\color{blue}\rmfamily}
% ---- Statistics ----- %
% abstract: 132
% Introduction: 554
% Related Work: 589

\begin{document}
    \title{Automatic Evaluation of Generative Dialogue Systems: An Empirical Study}
    \titlerunning{Automatic Evaluation of Generative Dialogue Systems}
    \author{Cong Feng\inst{1} \and Wenge Rong\inst{1} \and Jianxin Yang \inst{1} \and Yuanxin Ouyang \inst{1} \and Zhang Xiong \inst{1}}
    \authorrunning{C. Feng et al.}
    \institute{
    School of Computer Science and Engineering, Beihang University, Beijing, China
    \email{\{congfeng,w.rong,yjx17,oyyx,xiongz\}@buaa.edu.cn}
    }

    \maketitle

    % --------------------------- %
    % --------- Abstract -------- %
    % --------------------------- %
    \begin{abstract}
        Evaluating generative dialogue systems with automatic metrics is a difficult problem. It has been shown that the word-overlap metrics such as BLEU and the word-embedding metrics correlate poorly with human judgments. In this paper, we highlight yet another problem of evaluation with metrics through an empirical study. We show that regardless of the correlation with human judgments, the scores of these metrics do not correlate well with one another, which means fine-tuning the system on one set of metrics may yield a gotcha when testing with another set of metrics. This infeasibility again adds to the ineffectiveness of evaluation done alone with metrics. To the end of utilizing metrics at its best, we offer several pragmatic suggestions on the use of automatic metrics to avoid the issue of metric inconsistency.
        \keywords{Automatic Metrics, Dialogue Response Generation, Chatbot}
    \end{abstract}

    % ----------------------------------- %
    % ---------- Introduction ----------- %
    % ----------------------------------- %
    \section{Introduction}
    The chat-oriented dialogue systems have seen a boom in the recent literature of dialogue systems. These systems are trained to make an appropriate response given a conversational context and can be applied to various chatbot applications, language education tools, and intelligent personal assistants. This setting is different from that of the traditional task-oriented dialogue systems, which are programmed to assist their users on completion of a specific task, such as restaurant reservation and airline information acquisition. As a result, the chat-oriented dialogue systems cannot be evaluated in the same supervised setting as their task-oriented counterpart, which turns out to bring a new challenge.

    As an advanced approach to the chat-oriented system, the generative models based on neural networks feature end-to-end training with minimal hard-coded features and the ability to output fluent human-like sentences. Given a large corpus of context-response pairs, these neural models can learn language patterns and extract common-sense knowledge from the corpus in an unsupervised manner, making it a highly plausible solution to the response generation problem.

    The research community has made steady progress with the generative models. Vinyals et al. \cite{GoogleChatbot} applied the Seq2Seq model \cite{Seq2Seq} to generate responses to both technical queries in an IT helpdesk and open-ended questions such as \textit{what's the purpose of life?} Their model consistently outperformed a rule-based chatbot. However, generative models are not without problems. They incline to give meaningless responses to the questions they are incapable to handle, such as \textit{I don't know what you're talking about} and like any chat-oriented systems, their evaluation remains an open problem.

    Previous works of generative dialogue systems start by borrowing automatic metrics from the machine translation community, such as the popular BLEU \cite{BLEU} and METEOR \cite{METEOR}. However, the correlation between these borrowed metrics and human judgments remains unclear and researchers generally fall back to human evaluation for better accuracy and reliability \cite{Shang,DCGM,VHRED}. Liu et al. performed an empirical study on how well these automatic metrics correlate with human judgments \cite{HowNot}. They concluded that the metrics only correlated weakly with human judgments on the non-technical Twitter corpus and not at all on the technical Ubuntu Dialogue Corpus \cite{ubuntu_corpus}. They urged against the use of these metrics and called for the development of new automatic metrics that are more relevant to human judgments.

    In this paper, we extend the work of Liu et al.'s. and investigate the behaviors of automatic metrics when human judgments are not available. In particular, we highlight the drawbacks of existing metrics by trying to compare system performance purely based on them and show how inconsistent the results can be. Besides, we draw samples from the per-utterance scores of different metrics and analyze the pairwise correlation of these samples. We find that some pairs of samples have a high correlation, while other pairs show a much lower one.

    Following these observations, we cluster the metrics based on the pairwise correlations of their corresponding samples. The results show that metrics based on the same set of features tend to cluster together, indicating a high pairwise correlation within the same group. According to the experiment results, we argue that the scores from different metrics have the risk of high inconsistency, which is yet another pitfall of using automatic metrics. Therefore, it is strongly recommended to choose consistent metrics for the ease of comparison across different settings.

    % ----------------------------------- %
    % ----------- Related Work ---------- %
    % ----------------------------------- %
    \section{Related Work}
    Previous works on automatic metrics have mostly focused on the correlation between the metrics and human judgments. After all, the advantages of automatic metrics would be largely compromised if they cannot reflect human judgments. For example, in machine translation, where automatic metrics have been established to reduce human labor, almost every newly proposed metrics report an improved human correlation, as in \cite{NIST}, \cite{METEOR}, and \cite{chrf}.

    In dialogue response generation, to address the issue of low metric-human correlation, various semantic-based methods have been proposed. For example, the ADEM metric proposed by Lowe et al. models the human judgments with a feed-forward neural network \cite{ADEM}. Their concise architecture achieves a high correlation with human scores in various settings. The RUBER metric proposed by Tao et al. takes an asymmetric approach w.r.t the response-context pair and response-reference pair, where the former is modeled by a neural network and the latter is measured by an embedding-based metric. They combined two scores with various heuristics and achieved improvement on a Chinese corpus \cite{RUBER}. In brief, correlation with human judgments has always been the supervised signal guiding the evolution of automatic metrics.

    On the other hand, automatic metrics have been constantly doubted of their capability to reflect human judgments. In one of the earliest attempts to the response generation problem, Ritter et al. made an initial examination on the suitability of BLEU to this field leveraging the human data they collected \cite{Ritter11}. They found that the BLEU scores were very low even on the system level and the correlation was modest. Shang et al. argued that BLEU did not apply as the reasonable responses are too vast for the references to cover \cite{Shang}.

    Another popular metric is perplexity, which is generally used to evaluate statistical language models. Vinyals et al. showed that their Seq2Seq dialogue model achieved a much lower perplexity than the n-gram baseline, but they also admitted the drawbacks of using such a metric \cite{GoogleChatbot}. Serban et al. also used perplexity to evaluate their models in \cite{HRED}, along with other metrics. Like in \cite{GoogleChatbot}, they were not clear about how well these metrics accounted for the grammatical correctness and semantic coherence of the responses.

    An extensive study of metric-human correlation was conducted by Liu et al. \cite{HowNot}. Their study took into account of various settings, such as both retrieval and generative models and both technical and non-technical datasets. It revealed that although these metrics can distinguish state-of-art models from baseline models, none of them correlate highly with human scores. Their work has left us two questions towards better automatic metrics:
    \begin{enumerate}
        \item How can we improve the metric-human correlation?
        \item Why do the existing metrics correlate badly with human judgments?
    \end{enumerate}
    Previous works that proposed enhanced metrics endeavored to answer the first question, while we try to shed some light on the second one. In particular, we are curious about what can we learn when the scores from different metrics are put together and compared across various settings.

    To understand why some metrics correlate poorly with human judgments, we show that there are general inconsistencies among these metrics. Intuitively, if a set of metrics have low pairwise correlations, it is less likely that all of them would correlate highly with human judgments, which indicates that some of them must correlate poorly with human judgments. Along this line of thought, we arrive at an intermediate explanation to the poor performance of automatic metrics, backed by an empirical study. With this in mind, we can partially understand why these metrics fail and make improvements from the lesson we learned.

    % ----------------------------------- %
    % --------- Experiment Setup -------- %
    % ----------------------------------- %
    \section{Experiment Setup}
    Our experiment essentially involves training multiple generative models on multiple datasets, and then measure their performances with various automatic metrics. As stated, our study does not involve human evaluation. We also do not consider retrieval models as our interest falls on the more expressive generative models. Due to the constrains on time and resources, we limit our scope to three models and three datasets.

    To ease our demonstration, we define some symbols. A \emph{model} denoted as $m$ is an architecture of generative models, not an \emph{instance} of model trained on a specific dataset. A \emph{dataset} denoted as $d$ is a collection of context-response pairs from a specific domain, such as a social media or a technical support forum. A model predicts the response based on the context. A \emph{metric} denoted as $s$ is a function mapping a segment of words to a real value. The segment can be a sentence or a collection of sentences. The instance of model $m$ trained on dataset $d$ is denoted as $(m, d)$.

    % ------------------------- %
    % --------- Metrics ------- %
    % ------------------------- %
    \subsection{Metrics}
    We employ largely the same set of metrics studied by \cite{HowNot}, adding some novel ones from the recent literature. The word-overlap and word-embedding metrics, despite their issues with response generation, are studied again for more insights to their low performaces.

    \paragraph{BLEU.}
    BLEU \cite{BLEU} is a classical metric in the machine translation field that reports a high correlation with human scores on the system level. It owes the quality of a hypothesis to its similarity to a number of references and computes the similarity by accumulating n-gram precision of consecutive orders with the geometric mean, normalized by a brevity penalty.
    \begin{align}
        p_n &= \frac{ |C_n \cap R_n| }{ |R_n| } \\
        \textit{BP} &=
        \begin{cases}
            \ 1 \ & \text{if} \  c > r \\
            \ e^{1 - r/c} \ & \text{otherwise} \\
        \end{cases} \\
        \textit{BLEU} &=
        \textit{BP} \cdot \exp \left( \sum_{n=1}^N w_n \log p_n \right)
    \end{align}
    $p_n$ is the n-gram precision for order $n$. $C_n$ is a multiset of n-gram from a hypothesis while $R_n$ is the union of multisets of n-gram from each reference. $\textit{BP}$ is the brevity penalty. $c$ and $r$ are the length of the hypothesis and the effective length\footnote{The effective length of a collection of references is the length closest to that of the hypothesis.} of the references, respectively. $N$ is the total number of orders to compute n-gram precision for and a common choice is 4. $w_n$ is a weighting factor that usually takes the uniform weight $1 / N$.

    \paragraph{METEOR.}
    METEOR \cite{METEOR} is a metric proposed to address several issues with BLEU. It applies multiple stages of unigram matching to the hypothesis and reference, each using a different criterion, such as exact matching, WordNet synonyms, and paraphrases. An alignment is then created from these unigram matches. The score is based on the F1 score of the alignment and a penalty on shorter matches.

    \paragraph{ROUGE.}
    ROUGE \cite{ROUGE} is a family of metrics for automatic summarization. It defines a framework based on the F1 score and can integrate different matching units. For example, ROUGE-N uses n-gram as matching unit and ROUGE-L uses the longest common subsequence. The common form of ROUGE is:
    \begin{align}
        \textit{ROUGE} = \frac{
        R \cdot P
        }{(1 - \alpha) P + \alpha R}
    \end{align}
    with $R$ being the recall and $P$ being the precision. $\alpha$ controls the relative importance of $R$ and $P$, and it should be a large value if only recall is considered.

    \paragraph{Embedding Average.}
    Embedding Average is a metric based on word embedding, a distributed approach to the meaning of words \cite{word2vec}. The embedding of a sentence is defined as the average of the embeddings of its composing words. The similarity of two sentences is then simply defined as the cosine of the corresponding vectors:

    \paragraph{Vector Extrema.}
    Vector Extrema \cite{Vector_Extrema} composes the sentence embedding by taking the most extreme value (either maximum or minimum) along each dimension from its constituted words. The intuition is that in the embedding space, common words like function words are pulled towards the origin as they appear in the context of many different words, while informative words are pushed away from the origin in either positive or negative direction, since they tend to appear in more specific context. By taking the most extreme values from the embeddings of the constituted words, this method essentially captures the most informative words of a sentence and ignore common words.

    \paragraph{Greedy Matching.}
    Greedy Matching \cite{GreedyAndOptimal} is an embedding-based method without explicitly calculating a sentence vector. Instead, the two sentences under comparison are treated as a weighted bipartite graph, with their words as nodes and the embedding cosine of a pair of words $(w, w')$ as the weight of this edge. Greedy Matching is based on a greedy method to solve the optimal matching problem:
    \begin{align}
        G(r, \hat{r}) = \frac{
        \sum_{w \in r} \max_{\hat{w} \in \hat{r}} \cos(e_w, e_{\hat{w}})
        }{ |r| } \\
        \textit{Greedy-Matching} = \frac{
        G(r, \hat{r}) + G(\hat{r}, r)
        }{2}
    \end{align}
    where $r$ and $\hat{r}$ are the reference and response, respectively.

    \paragraph{ADEM}
    ADEM \cite{ADEM} is based on a nueral network, which is trained on a conversation corpus with human-annotated scores. The model first embeds the input into a low-rank vector space and then predicts a human score with a linear combination of the embeddings for a context $c$, response $\hat{r}$, and reference $r$.
    \begin{align}
        s(c, r, \hat{r}) = \frac{(c^T M \hat{r} + r^T N \hat{r} - \alpha)}{\beta}
    \end{align}
    $M, N$ are learnable parameters of the network and $\alpha, \beta$ are constants to normalize the output of the network to the five-point score. The model is trained end-to-end to minimize a squared error with a L2 regularization term:
    \begin{align}
        L = \sum_{i=1}^{K} (s_i - h_i)^2 + \gamma \left\| \theta \right\| _2
    \end{align}
    where $K$ is the number of samples in a batch. $s_i$ and $h_i$ are the score predicted by the model and the score labeled by human, respectively. $\theta$ is the parameters of the network and $\gamma$ is the regularization factor.

    \paragraph{Distinct-N.}
    Distinct-N \cite{MMI} measures the rate of unique n-grams of a sentence. For a sentence, it is the size of the set of the n-grams (each n-gram appears only once) divided by the total number of n-grams. It measures the diversity of a response at the token level.

    % ------------------------- %
    % --------- Models -------- %
    % ------------------------- %
    \subsection{Models}
    We choose the models used in \cite{VHRED}, namely LSTM, HRED, and VHRED. These models are either derived from the LSTM recurrent neural network \cite{LSTM} or extensions to the standard Seq2Seq framework. As such, they are ideal representatives of the generative models and have been widely used as baselines in various settings.

    \paragraph{LSTM.}
    The LSTM model is a simple generative model with a single RNN acting as both an encoder and a decoder. For an input sequence $X = x_1, x_2, \dots, x_n$ and an output sequence $Y = y_1, y_2, \dots y_m$, the model predicts $y_t$ conditioned on $x_t$ at time step $t$, as shown in figure \ref{fig:LSTM_as_generative}. Note that the generative LSTM is not an autoregressive model as the output of the previous time step does not become the input of the current time step.
    \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{figure/LSTM/generative.pdf}
        \caption{LSTM as a generative model}
        \label{fig:LSTM_as_generative}
    \end{figure}

    \paragraph{HRED.}
    The HRED model \cite{hred-qs,HRED} features a hierarchical encoding mechanism that takes into account the hierarchical structure of dialogues. For a dialogue $D$ with $M$ sentences: $D = U_1, U_2, \dots, U_M$, where each sentence $U_m$ consists of $N_m$ tokens: $U_m = w_{m, 1}, w_{m, 2}, \dots, w_{m, N_m}$, HRED first encodes each sentence with a \emph{utterance encoder} into a fixed-length utterance vector $e_u$. Then, these utterance vectors are processed iteratively by the \emph{context encoder} to produce a fixed-length context vector $e_d$. Finally, the \emph{utterance decoder} takes the context vector and generates the next utterance of the dialogue. With the hierarchical generation process, HRED can better incorporate long-term dialogue history and generate more topic-relevant resposnes \cite{A_Short_Review}.

    \paragraph{VHRED.}
    The VHRED model \cite{VHRED} extends the HRED model with a variational inference mechanism. Essentially, VHRED injects into the utterance decoder a random variable sampled from a high-dimension normal distribution, whose parameters are conditioned on the context vector $e_d$. With the randomness at the decoder, VHRED is able to model the uncertainty and ambiguity in the dialogues, generating more diverse responses.

    % --------------------------- %
    % --------- Datasets -------- %
    % --------------------------- %
    \subsection{Datasets}
    To observe a general phenomenon, we choose datasets from a variety of domains. Specifically, the technical domain is selected because it contains a lot of rare words and requires technical knowledge. The movie domain is selected for its fiction nature and dependencies on story backgrounds. The forum domain is selected since it reflects the disscusion around a specific topic with multiple participants.

    \paragraph{Ubuntu Dialogue Corpus}
    The Ubuntu Dialogue Corpus \cite{ubuntu_corpus} is a large-scale multi-turn dyadic technical corpus collected from the Ubuntu channel of an IRC network. It contains a lot of technical symbols, such as filesystem paths, commands, and URLs. Its technical nature with large number of examples helps construct data-driven applications for technical support.

    \paragraph{OpenSubtitles}
    The OpenSubtitles dataset \cite{opensub} is a large-scale monolingual corpus of movie subtitles. It is collected by the OPUS project \cite{OPUS} from the \emph{opensubtitles} website\footnote{\url{http://www.opensubtitles.org}}. It is noisy as its examples are inherently not of dialogue structure. In other words, there are no explicit turn-taking information or dialogue boundaries in it. Typically, the consecutive two utterances are treated as a context-response pair, as in \cite{GoogleChatbot}, \cite{MMI}, and \cite{persona}.

    \paragraph{LSDSCC}
    LSDSCC \cite{LSDSCC} is a \emph{Large Scale Domain-Specific Conversation Corpus} collected from the movie subreddit of the Reddit forum. It is believed that a more specific domain can help avoid generating universal responses \cite{LSDSCC}. The authors applied dedicated cleansing procedures to the raw data to retain as much information as possible. They also used human annotations to ensure the quality of the corpus.

    % ------------------------------------- %
    % --------- Experimental Study -------- %
    % ------------------------------------- %
    \section{Experimental Study}
    \subsection{System-Level Scores}
    We first measured the system-level scores for all settings, shown in table \ref{tab:system_scores}. The system-level score reflects the average performance of a model trained on a dataset. For metrics that do not have an explicit system-level definition, we took the arithmetic average of their example-level scores.
    \input{data/system_scores.tex}

    The system-level scores of different metrics look quite consistent in terms of the best performing model on a certain dataset. On LSDSCC, for example, the HRED model beats other models on all but one metrics (ROUGE-4), while on OpenSubtitles, the VHRED model wins the best of most of the metrics. The system-level score seems to be able to distinguish state-of-the-art models from baselines, as most of the metrics agree on which model is the best.

    However, the consistency among the metrics does not necessarily lead to a high correlation with human judgment, as observed in \cite{HowNot}. Moreover, the system-level score is calculated by accumulating over the example-level scores. Thus, it is possible that the inconsistency on the example-level is hidden away. To reveal the possible inconsistency, we performed analyses on the example-level scores.

    \subsection{Example-Level Scores}
    \input{data/heatmap.tex}
    On the example level, a matrix $M \in R^{N \times M}$ is calculated for each model instance, where $N$ is the number of examples and $M$ is the number of metrics. Each row $r_i$ of the matrix is the scores of all metrics for an example $e_i$, while each column $c_j$ is the values of a metric $s_j$ computed for all examples. All the metrics share the same set of examples within a matrix. Thus, the correlation of any two columns $c_i, c_j$ can be understood as to how much the corresponding metrics $s_i, s_j$ agree on their shared examples. We compute the pairwise correlations of the metrics and highlight the degree of correlation with heatmaps, as shown in figure \ref{fig:corr_heatmap}. We used Pearson's r and all the results are statistically significant.

    Each subfigure of figure \ref{fig:corr_heatmap} is plotted from the pairwise correlations of scores of a model instance. The color of the cells represent the degree of correlation between the row and column label, with red, blue, and white stand for possitive, negative and zero correlation, respectively.

    From the brighter regions, we can discover some groups of metrics with high correlations inside each group. Specifically, we find that metrics based on word overlap tend to have high pairwise correlations, which can be explained by the fact that they all make use of the n-gram statistics somehow. On the other hand, it is confirming to observe that the ADEM metric, with a much higher correlation with human judgment, does not correlate with all the other metrics, which is naturally true due to the transitivity of the correlation relation.

    To better observe the agreement and disagreement among the metrics, we applied hierarchical clustering to the columns of the matrix and the results are shown in figure \ref{fig:hierarchy}. A hierarchical clustering algorithm starts with a forest of nodes and iteratively merges them into larger clusters until the root cluster is found. The node-level distance $\textit{dist}(\cdot, \cdot)$ and cluster-level distance $d(\cdot, \cdot)$ we used are defined as follow:
    \begin{align}
        \textit{dist}(i, j) &= 1 - \textit{corr}(i, j) \\
        d(u, v) &= \frac{\sum_{i,j}\textit{dist}(i, j)}{|u| \cdot |v|}
    \end{align}
    where $i$ and $j$ are points in cluster $u$ and $v$, respectively. $|u|$ and $|v|$ are the cardinalities of cluster $u$ and $v$, respectively\footnote{This is also known as the average method.}. We tried three correlation methods, namely Pearson's r, Spearman's r and Kendall's $\tau$ and observe similar results. For the sake of space, only the results of Pearson's r are displayed.

    From figure \ref{fig:hierarchy}, we observe a hierarchy of agreement among different metrics. The clusters that were created in the earlier iterations appear closer to the leaves of the tree, which indicates higher pairwise correlations or stronger inter-metric agreements. Across all the settings, we generally observe these independent clusters:
    \begin{enumerate}
        \item Word-Overlap: a large cluster containing many subclusters formed by different word-overlap metrics, such as BLEU, ROUGE, and METEOR.
        \item Word-Embedding: a small cluster that contains Vector-Average, Vector-Extrema, and Greedy-Matching.
        \item Distinct-2 and \#words: a generally observable cluster, with underlying reason yet to know.
        \item ADEM: a standalone cluster formed by ADEM due to its high metric-human correlation.
        \item Distinct-1: a metric that might belong to some cluster or become a standalone cluster depending on the dataset involved.
    \end{enumerate}
    \input{data/hierarchy.tex}

    Those clusters confirm our hypothesis on the inconsistency among different metrics. Luckily, there are still some forms of agreement that the same cluster of metrics can reach, due to odds or intrinsic reasons. Unfortunately, the exact reasons why the clusters are formed this way remain unclear. Although our experiments have a simple procedure, it should be noted that the mechanisms behind each step are very complex. For example, the dialogue datasets are intrinsically very diverse and the way the generative models work is not well-understood. Besides, it is also unclear how these metrics reflect the desirable properties of a response. Hence we can only conclude that similar metrics tend to be consistent on the example-level. The similarity of metrics generally refers to their mechanisms, such as the way they extract overlap unit or semantics from an utterance or the way different components are combined.

    \subsection{Qualitative Analysis}


    \section{Discussion}
    Conclusions from the experiment.
    Advice on using metrics.

    \subsection*{Acknowledgements.}
    This work was done when the author has just graduated from Beihang University. The continuous support from the laboratory of Beihang University is the key to the completion of the work. The author also appreciates the encouragement and advice from his tutor Wenge Rong, an associate professor at Beihang University.

    \bibliographystyle{splncs04}
    \bibliography{data/all}
\end{document}
