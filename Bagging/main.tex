Abstract. Feature learning plays an important role in many machine
learning tasks. As a common implementation for feature learning, the
auto-encoder has shown excellent performance. However it also faces
several challenges among which a notable one is how to reduce its gen-
eralization error. Different approaches have been proposed to solve this
problem and Bagging is lauded as a possible one since it is easily im-
plemented while can also expect outstanding performance. This paper
studies the problem of integrating different prediction models by bag-
ging auto-encoder-based classifiers in order to reduce generalization error
and improve prediction performance. Furthermore, experimental study
on different datasets from different domains is conducted. Several inte-
gration schemas are empirically evaluated to analyse their pros and cons.
It is believed that this work will offer researchers in this field insight in
bagging auto-encoder-based classifiers. (699)

Keywords: bagging, auto-encoders, feature learning, neural networks.

Introduction.
When it comes to feature learning, there are two commonly used approaches,
i.e., handcrafting features learning and unsupervised features learning [15]. Com-
pared with manually selecting features, which is arduous and requires several
validation tests to determine which features are the most representative [10],
unsupervised features learning employs learning models to obtain appropriate
features. In recent years neural network based feature learning model has been
attached much importance as it allows users to perform unsupervised feature
learning with unlabelled data [12] and one of the popular learning models is
stacked auto-encoder [7].

To build a stacked auto-encoder-based feature learning model, a common
approach is to add one classification layer on the top of stacked auto-encoder
and then take the learned features as input for the classification layer [9]. Though
the auto-encoder-based classification model has many advantages, it still faces
several challenges among which a notable one is how to reduce generalization
error [4]. To meet this challenge, a lot of approaches have been proposed among
which widely adopted ones are ensemble methods [13].

Bagging (Bootstrap Aggregation) is a popular ensemble method which con-
sists in bootstrapping several copies of the training set and then employing
them to train several separate models. Afterwards it combines the individual
predictions together by a voting scheme for classification applications [5]. As
each bootstrapped training set is slightly different from each other, each model
trained on theses training sets has different weights and different focus, thereby
having different generalization error. By combining them together, the overall
generalization error is expected to decrease to some extent.

Previous works have shown that bagging works well for unstable predictors
[14]. Considering neural-network-based models are also unstable predictors [19],
it is intuitive to assume that applying bagging methods to feature learning based
models could also probably improve classification performance. Therefore a fun-
damental question is arisen accordingly: is it possible to integrate feature learning
with ensemble method such as bagging? If so, how can we effectively integrate
them? In this research, we thoroughly investigate the possibility of integrating
feature learning with bagging ensemble method and further provide different
integration schemas and empirically evaluate their pros and cons.
The remainder of this paper is organised as follow. In section 2 we introduce
the background about auto-encoder and bagging techniques. Section 3 presents
the proposed evaluation architecture. In section 4 we will present the experimen-
tal settings and also discuss the experimental results. Finally section 5 concludes
the paper and points possible future work. (973)
