\documentclass[runningheads]{llncs}

\usepackage{graphicx}
\usepackage{array}
\usepackage{booktabs}
\usepackage{float}
\usepackage{hyperref}
\usepackage{amsmath}

\renewcommand\UrlFont{\color{blue}\rmfamily}
% ---- Statistics ----- %
% abstract: 132
% Introduction: 554
% Related Work: 589

\begin{document}
    \title{Yet Another Problem of Automatic Evaluation of Generative Dialogue Systems}

    \titlerunning{Yet Another Problem}

    \author{Cong Feng\inst{1} \and Wenge Rong\inst{1}}

    \authorrunning{C. Feng et al.}

    \institute{
    School of Computer Science and Engineering, Beihang University, Beijing, China
    \email{cgsdfc@126.com} \\
    \email{w.rong@buaa.edu.cn}
    }

    \maketitle

    \begin{abstract}
        Evaluating generative dialogue systems with automatic metrics is a difficult problem. It has been shown that metrics based on word overlap and word embedding correlate poorly with human judgments. In this paper, we highlight yet another problem of evaluation done with metrics through an empirical study. We show that regardless of the correlation with human judgments, the scores of the metrics do not correlate well with one another, which means fine-tuning the system on one set of metrics may yield a gotcha when testing with another set of metrics. This infeasibility again adds to the ineffectiveness of evaluation done alone with metrics. To the end of utilizing metrics at its best, we offer several pragmatic pieces of advice on the use of automatic metrics to avoid the issue of metric inconsistency.
        \keywords{Automatic Metrics, Response Generation, Chatbot}
    \end{abstract}


    \section{Introduction}
    The chat-oriented dialogue systems have seen a boom in the recent literature of dialogue systems. These systems are trained to make an appropriate response given a conversational context and can be applied to various chatbot applications, language education tools, and intelligent personal assistants. This setting is different from that of the traditional task-oriented dialogue systems, which are programmed to assist their users on completion of a specific task, such as restaurant reservation and airline information acquisition. As such, the chat-oriented dialogue systems cannot be evaluated in the same supervised setting as their task-oriented counterpart, which turns out to bring a new challenge.

    As an advanced technique to implement the chat-oriented system, the generative model based on neural network architecture feature end-to-end training with minimal hard-coded features and fluent human-like output sentences. Given a large corpus of context-response pairs, these neural models can learn language patterns and extract common-sense knowledge from the corpus in an unsupervised manner, making it a highly plausible solution to the response generation problem. The research community has made steady progress with the generative models. Vinyals et al. \cite{GoogleChatbot} apply the Seq2Seq model \cite{Seq2Seq} to generate responses to both technical queries in an IT helpdesk and open-ended questions such as \textit{what's the purpose of life?} and their model consistently beats a rule-based chatbot. However, generative models are not without problems. They inclined to give a meaningless response to the questions they are incapable to handle, such as \textit{I don't know what you're talking about} and like any chat-oriented systems, their evaluation remains an open problem.

    Previous works of generative dialogue systems start by borrowing automatic metrics from the machine translation community, such as the popular BLEU \cite{BLEU} and METEOR \cite{METEOR}. However, the correlation between these borrowed metrics and human judgments remains unclear and researchers generally turn back to human evaluation for better accuracy and reliability \cite{Shang,DCGM,VHRED}. Liu et al. perform an empirical study on how well these automatic metrics correlate with human judgments \cite{HowNot}. They conclude that the metrics only correlate weakly with human judgments on the non-technical Twitter corpus and not at all on the technical Ubuntu Dialogue Corpus \cite{ubuntu_corpus}. They urges against the use of these metrics and calls for the development of new automatic metrics that are more relevant to human judgments.

    In this paper, we extend the work of Liu et al. and investigate the behaviors of automatic metrics when human judgments is not available. In particular, we highlight the drawbacks of existing metrics by trying to compare system performance purely based on them and show how inconsistent the results can be. Besides, we draw samples from the per-utterance scores of different metrics and analyze the pairwise correlation of these samples. We find that some pairs of samples have a high correlation, while other pairs show a much lower one. Following this observation, we cluster the metrics based on the correlation of their corresponding samples. The results show that metrics based on the same set of features tend to cluster together, indicating a high pairwise correlation within the same group. Our experimental results show yet another pitfall of using automatic metrics that the results from different metrics have the risk of high inconsistency if they don't happen to be in the same group. We therefore strongly advise researchers to wisely choose their metric to use to promote the comparison across different models and datasets.

    \section{Related Work}
    Previous works on automatic metrics have mostly focused on their correlation with human judgments. After all, the advantages of automatic metrics would be largely compromised if they cannot reflect human judgments. In machine translation, where automatic metrics have been established to reduce human labor, almost every newly proposed metrics report an improved human correlation, as in \cite{NIST}, \cite{METEOR}, and \cite{chrf}. In dialogue response generation, to address the issue of low metric-human correlation, various semantic-based methods have been proposed. For example, the ADEM metric proposed by Lowe et al. fits a linear combination of a reference, context, and response with a feed-forward neural network to predict a five-point human score \cite{ADEM}. This concise architecture achieves a high correlation with human scores in various settings. The RUBER metric proposed by Tao et al. takes an asymmetric approach w.r.t the response-context and response-reference relationships, where the former is modeled by a neural network and the latter is measured by an embedding-based metric. They combine two scores using various heuristics and achieve improvement on a Chinese corpus \cite{RUBER}. In brief, correlation with human judgments has always been the supervised signal guiding the evolution of automatic metrics.

    On the other hand, automatic metrics have been constantly doubted of their capability to reflect human judgments. In one of the earliest attempts to the response generation problem, Ritter et al. make an initial examination on the suitability of BLEU to this field leveraging the human data they collect \cite{Ritter11}. They found the BLEU scores were very low even on the system level and the correlation is modest. Shang et al. argue that BLEU does not apply as the reasonable responses are too vast for the references used in BLEU to cover \cite{Shang}. Another popular metric is perplexity, which is generally used to evaluate statistical language models. Vinyals et al. show that their Seq2Seq dialogue model has a much lower perplexity than the n-gram baseline, but they also admit the drawbacks of using such a metric \cite{GoogleChatbot}. Serban et al. also use perplexity to evaluate their models in \cite{HRED}, along with other metrics. Like in \cite{GoogleChatbot}, they are not clear about how well these metrics account for the grammatical correctness and semantic coherence of the responses.

    An extensive study of metric-human correlation is conducted by Liu et al. \cite{HowNot}. The study takes into account of various settings, such as both retrieval and generative models and both technical and non-technical datasets. It reveals that although these metrics can distinguish state-of-art models from baseline models, none of them correlate highly with human scores. Their work has left us two problems towards better automatic metrics:
    \begin{enumerate}
        \item How can we improve the metric-human correlation?
        \item Why do the existing metrics correlate badly with human judgments?
    \end{enumerate}
    Most previous works that propose enhanced metrics endeavor to solve the first problem, while we try to shed some light on the second problem. In particular, we are curious about what can we learn when the scores from different metrics are put together and compared across various settings.

    To understand why some metrics correlate poorly with human judgments, we try to show that there is a general inconsistency among these metrics. Intuitively, if a set of metrics have low pairwise correlations, it is less likely that all of them will correlate with human judgments highly, which indicates that some of them must correlate poorly with human judgments. Along this line of thought, we obtain an intermediate explanation to the poor performance of automatic metrics, backed by an empirical study. With this in mind, we can partially understand why these metrics fail and make improvements from the lesson we learned.

    \section{Experiment Setup}
    Our experiment essentially involves training multiple generative models on multiple datasets, and then measure their performance with various automatic metrics. As stated, our study does not involve human evaluation. We also do not consider retrieval models as our interest falls on the more expressive generative models. Due to the constrains on time and resources, we limit our scope of investigation to three models and three datasets.

    To ease our demonstration, we define some symbols. A \emph{model} denoted as $m$ is an architecture of generative models, not an \emph{instance} of model trained on a specific dataset. A \emph{dataset} denoted as $d$ is a collection of context-response pairs from a specific domain, such as a social media or a technical support forum. A model predicts the response based on the context. A \emph{metric} denoted as $s$ is a function that maps a segment of words to a real value. The segment can be a sentence or a collection of sentences. The model instance of architecture $m$ trained on dataset $d$ is denoted as $(m, d)$.

    \subsection{Metrics}
    We employ largely the same set of metrics studied by \cite{HowNot}, adding some novel ones found in the literature. The word overlap metrics, in spite of their issues with response generation, are still included as a baseline.

    \paragraph{BLEU.}
    BLEU \cite{BLEU} is a classical metric in the machine translation field that reports a high correlation with human scores on the system level. It owes the quality of a hypothesis to its similarity to a number of references and computes the similarity by accumulating n-gram precision of consecutive orders with geometric mean, normalized by a brevity penalty.
    \begin{align}
        p_n &= \frac{ |C_n \cap R_n| }{ |R_n| } \\
        \textit{BP} &=
        \begin{cases}
            \ 1 \ & \text{if} \  c > r \\
            \ e^{1 - r/c} \ & \text{otherwise} \\
        \end{cases} \\
        \textit{BLEU} &=
        \textit{BP} \cdot \exp \left( \sum_{n=1}^N w_n \log p_n \right)
    \end{align}
    $p_n$ is the n-gram precision for order $n$. $C_n$ is a multiset of n-gram from a hypothesis while $R_n$ is the union of multisets of n-gram from each reference. $\textit{BP}$ is the brevity penalty. $c$ and $r$ are the lengths of the hypothesis and reference, respectively. $N$ is the total number of orders to compute n-gram precision for and a common choice of it is 4. $w_n$ is a weighting factor that usually takes the uniform weight $1 / N$.

    \paragraph{METEOR.}
    METEOR \cite{METEOR} is a metric proposed to address several issues with BLEU. It applies multiple stages of unigram matching to the hypothesis and reference, each using a different criterion, such as exact matching, WordNet synonyms, and paraphrases. An alignment is created from the unigram matches and the final score is based on the F1 score of the alignment and a penalty on shorter matches.

    \paragraph{ROUGE.}
    ROUGE \cite{ROUGE} is a family of metrics for automatic summarization. It defines a framework based on the F1 score and can integrate different matching units. For example, ROUGE-N uses n-gram as matching unit and ROUGE-L uses the longest common subsequence. The common form of ROUGE takes that of the F1 score
    \begin{align}
        \textit{ROUGE} = \frac{
        R \cdot P
        }{(1 - \alpha) P + \alpha R}
    \end{align}
    with $R$ being the recall and $P$ being the precision. $\alpha$ controls the relative importance of them, and it should be a large value if only recall is considered.

    \paragraph{Embedding Average.}
    Embedding Average is a metric based on word embedding, a distributed approach to the meaning of words \cite{word2vec}. The embedding of a sentence is defined as the average of the embeddings of its composing words. The similarity of two sentences is then simply defined as the cosine of the corresponding vectors.

    \paragraph{Vector Extrema.}
    Vector Extrema \cite{Vector_Extrema} composes the sentence embedding by taking the most extreme value (either maximum or minimum) along each dimension from its constituted words. The intuition is that in the embedding space, common words like function words are pulled towards the origin as they appear in the context of many different words, while informative words are pushed away from the origin in either positive or negative direction, since they tend to appear in more specific context. By taking the most extreme values from the embeddings of the constituted words, this method essentially captures the informative content of a sentence and ignore common words.

    \paragraph{Greedy Matching.}
    Greedy Matching \cite{GreedyAndOptimal} is an embedding-based method without explicit calculation of a sentence vector. Instead, the two sentences under comparison are treated as a weighted bipartite graph, with their words as nodes and the embedding cosine of a pair of words $(w, w')$ as the weight on this edge. Greedy Matching is based on a greedy method that finds the optimal match.
    \begin{align}
        G(r, \hat{r}) = \frac{
        \sum_{w \in r} \max_{\hat{w} \in \hat{r}} \cos(e_w, e_{\hat{w}})
        }{ |r| } \\
        \textit{Greedy-Matching} = \frac{
        G(r, \hat{r}) + G(\hat{r}, r)
        }{2}
    \end{align}
    where $r$ and $\hat{r}$ are the reference and response, respectively.

    \paragraph{ADEM}
    The ADEM \cite{ADEM} metric is based on a nueral network, which is trained on a conversation corpus with human-annotated scores. The model first embeds the input into a low-rank vector space and then predicts a human score with a linear combination of the embeddings for a context $c$, response $\hat{r}$, and reference $r$.
    \begin{align}
        s(c, r, \hat{r}) = \frac{(c^T M \hat{r} + r^T N \hat{r} - \alpha)}{\beta}
    \end{align}
    $M, N$ are learnable parameters of the network and $\alpha, \beta$ are constants to normalize the output of the network to the five-point score. The model is trained end-to-end minimizing a squared error objective function with L2 regularization:
    \begin{align}
        L = \sum_{i=1}^{K} (s_i - h_i)^2 + \gamma \left\| \theta \right\| _2
    \end{align}
    where $K$ is the number of samples in a batch. $s_i$ and $h_i$ are the score predicted by the model and the score labeled by human, respectively. $\theta$ is the parameters of the network and $\gamma$ is the regularization factor.

    \paragraph{Distinct-N.}
    Distinct-N \cite{MMI} measures the rate of unique n-grams of a sentence. For a sentence, it is simply the size of the set of the n-grams (each n-gram appears only once), divided by the total number of n-grams. It measures the diversity of a response at the token level.

    \subsection{Models}


    \paragraph{LSTM language model.}
    \paragraph{HRED.}
    \paragraph{VHRED.}

    \subsection{Datasets}
    Introduce datasets, one per paragraph.
    \paragraph{Ubuntu Dialogue Corpus}
    \paragraph{OpenSubtitles}
    \paragraph{LSDSCC}


    \section{Experimental Study}
    \paragraph{Statistical Analysis.}
    Display the clustering of metrics through multiple perspectives,
    such as the similarity of their distribution plots, tables of Pearson and Spearman Correlation
    and the visualization outcomes of a certain clustering algorithm.

    \paragraph{Qualitative Analysis.}
    Display and analyze some examples where different metrics disagree greatly.
    For example, a response with no n-gram overlap will get zero BLEU but if it is semantically
    relevant to the message or reference, embedding-based metric will give it a high score.

    \section{Discussion}
    Conclusions from the experiment.
    Advice on using metrics.

    \subsection*{Acknowledgements.}
    This work was done when the author has just graduated from Beihang University. The continuous support from the laboratory of Beihang University is the key to the completion of the work. The author also appreciates the encouragement and advice from his tutor Wenge Rong, an associate professor at Beihang University.

    \bibliographystyle{splncs04}
    \bibliography{data/all}

\end{document}
