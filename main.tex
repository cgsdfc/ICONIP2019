% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}
%
\begin{document}
\title{An Diversity-Oriented Analysis on Evaluating the Generative Dialogue Models}
%
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
\titlerunning{An Diversity-Oriented Analysis}
%
\author{Cong Feng\inst{1} \and Wenge Rong\inst{1}}
%
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
\authorrunning{C. Feng et al.}
%
\institute{School of Computer Science and Engineering,
Beihang University, Beijing, China}
%
% This is my email:
% \email{cgsdfc@126.com}
% This my tutor's email:
% \email{w.rong@buaa.edu.cn}
%
\maketitle
%
% Required: 150--250
% Actual: 585
% Shorten this abstract.
\begin{abstract}
    Although the Seq2Seq-based generative dialogue systems are
    able to generate natural and
    fluent responses,
    they have been known for preferring to generate
    simple and repeated responses.
    Towards the goal of generating diverse, meaningful and
    engaging dialogues,
    many researchers proposed methods to address the
    problems of low-quality responses.
    However, it is also known that the
    lack of good automatic evaluation metrics
    has led the field to rely heavily on human evaluation,
    which is expensive and unscalable.
    To better understand the pros and cons of various
    evaluation metrics,
    we trained three generative models on
    three public-available datasets and measured their
    performances with various metrics on both system level
    and utterance level.
    We found that there is no observable consistency among scores
    across various models, datasets and metrics.
    However, on a per-metric basis, datasets generally pose a larger impact on
    the scores than models do.
    In addition, the distributions of
    utterance-level scores tend to have similar shape if the corresponding metrics
    use the same set of features as measurement.
    We owe the experimental observations to the facts
    that the enormous space for reasonable responses
    makes the evaluation hard to tackle.
    Meanwhile, the ability of the models to generalize across
    various datasets remain highly enhanceable.
    Finally, the highly diversed distributions of scores
    of various metrics makes the results somehow confusing.
    Based on the empirical results, we pointed out several
    directions for future work, including using new model
    architectures, investigating the statistical nature of
    the open-domain dialogue datasets and
    developing dataset-specific metrics.
\keywords{Natural Language Processing \and
Response Generation \and
Chatbot \and
Evaluation Metrics.}
\end{abstract}
%
%
\section{Introduction}
\section{Background}
\section{Evaluation Architecture}
\section{Experimental Study}
\section{Conclusion and Future Work}
\subsection*{Acknowledgements.}
%
\bibliographystyle{splncs04}
\bibliography{data/all}
%
\end{document}
