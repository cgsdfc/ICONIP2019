\documentclass[runningheads]{llncs}
\usepackage{amsmath}

% 5 + 4 + 2 = non-exp + exp + refs
\begin{document}
\title{How NOT To Evaluate Your Dialogue System: An Empirical Study of
Unsupervised Evaluation Metrics for Dialogue Response Generation\thanks{
This is a summarization of the paper written by Cong Feng.
}}

\titlerunning{How NOT To Evaluate Your Dialogue System}

\author{Chia-Wei Liu \inst{1} \and Ryan Lowe \inst{1} \and
Iulian V. Serban \inst{2} \and Micheal Noseworthy \inst{1}}

\institute{School of Computer Science, McGill University
\email{\{chia-wei.liu,ryan.lowe,michael.noseworthy\}@mail.mcgill.ca}
\and
DIRO, Universite de Montreal \\
\email{iulian.vlad.serban@umontreal.ca}}

\maketitle

\begin{abstract}
    We investigate evaluation metrics for dialogue
    response generation systems where supervised
    labels, such as task completion, are not available.
    Recent works in response generation
    have adopted metrics from machine translation
    to compare a modelâ€™s generated response
    to a single target response. We show that
    these metrics correlate very weakly with human
    judgements in the non-technical Twitter
    domain, and not at all in the technical Ubuntu
    domain. We provide quantitative and qualitative
    results highlighting specific weaknesses
    in existing metrics, and provide recommendations
    for future development of better automatic
    evaluation metrics for dialogue systems.
\end{abstract}

\section{Introduction}
\paragraph{1.} Two kinds of dialogue response generation systems:
supervised dialogue models and unsupervised dialogue models.

\paragraph{2. } The focus of this paper,
the unsupervised models such as chatbots using end-to-end training
with neural networks can avoid supervised labels but their
evaluation is an open problem.

\paragraph{3. } Other NLP tasks have developed effective automatic metrics such as:
\begin{enumerate}
    \item Machine translation: BLEU and METEOR.
    \item Automatic Summarization: ROUGE.
\end{enumerate}
But the assumption these metrics make is too strong for dialogue systems, which
permit vastly diverse responses to a given context.

\paragraph{4. } The main work of this paper: investigate the correlation
between several metrics and human judgement on two datasets and four models.
The \textbf{experiment objects} of this paper is as follow:
\begin{enumerate}
    \item Models:
    \begin{enumerate}
          \item Retrieval: TF-IDF, Dual Encoder.
        \item Generative: LSTM, HRED.
    \end{enumerate}
    \item Metrics:
    \begin{enumerate}
        \item Word-Overlap: BLEU, METEOR, ROUGE.
        \item Word-Embedding: ...
    \end{enumerate}
    \item Datasets:
    \begin{enumerate}
        \item Chitchat-Oriented: Twitter dataset.
        \item Technical-Oriented: Ubuntu Dialogue Dataset.
    \end{enumerate}
\end{enumerate}

Major conclusion:
For all metrics, there is weak correlation on Twitter dataset and no correlation
on Ubuntu dataset.
Methodology:
\begin{enumerate}
    \item statistical analysis on the survey's results.
    \item qualitative analysis of examples.
    \item exploration of the sensitivity of the metrics.
\end{enumerate}

\paragraph{5. } Conclusion of the experiment:
\begin{enumerate}
    \item The advice to the researchers: shift away from these metrics.
    \item We need to find a new metric that correlates more strongly with human.
\end{enumerate}

\section{Related Work}
% Find good reasons why some of the metrics are not used.
\paragraph{1. } Excluding certain metrics and the reasons of doing so.
\begin{enumerate}
    \item Perplexity
    \begin{enumerate}
        \item Violate \emph{model-independent}. The model generating the response
        does not evaluate itself.
        \item There is no utterance level score for PPL.
        \item Cannot be computed for retrieval models.
    \end{enumerate}
    \item Recall
    \begin{enumerate}
        \item The pattern of metrics should be evaluating the response against the
        ground-truth (The setting in machine translation).
    \end{enumerate}
    \item Metrics for supervised models.
    \begin{enumerate}
        \item We don't study supervised metrics in this paper.
        \item These metrics are well-studied.
    \end{enumerate}
\end{enumerate}

\paragraph{2.} Usage of metrics in the lierature.
\begin{enumerate}
    \item Ritter et al. (2011) use an SMT model to beat retrieval baselines
    under BLEU.
    \item Sordoni et al. (2015) use an RNNLM model and context-sensitive multi-BLEU.
    \item Li et al. (2015) evaluate their diversity-promoting objective function
    with single-reference BLEU.
    \item Galley et al. (2015b) propose deltaBLEU that uses multiple human evaluated
    ground-truth. But it only has weak to moderate correlation to human judgement.
\end{enumerate}

\paragraph{3.} There are studies
of correlation of metrics with human judgements in other NLP fields,
such as MT and NLG. But dialogue response generation is intrinsically
a harder problem since the set of correct answers are large.
Studies are listed as follow:
\begin{enumerate}
    \item MT: (Callison-Burch et al., 2010; Callison-Burch et al.,
2011; Bojar et al., 2014; Graham et al., 2015).
    \item NLG: (Stent et
al., 2005; Cahill, 2009; Reiter and Belz, 2009; Es-
pinosa et al., 2010).
\end{enumerate}

\section{Evaluation Metrics}
\paragraph{1.} The goal and one of the mechanism of the evaluation metrics:
\begin{itemize}
    \item Goal: Automatically evaluate how appropriate the proposed response
    is to the conversation given its context.
    \item Mechanism: Compare the response to the conversation.
\end{itemize}
% The logic: We want to know how appropriate
% A is to B and we know C is appropriate to B so we reduce the problem to
% how A is similar to B.
% This is the so-called the ground-truth-based metric. It only considers the
% ground truth and do not directly or indirectly consider the context.

\subsection{Word Overlap-based Metrics}
\paragraph{1.} Word-Overlap metrics evaluate the amount of word-overlap between
the proposed response and the ground-truth.
BLEU and METEOR are for MT while ROUGE is for AS, all of which are shown to have
correlation to human judgements in their target domain, but not yet in our domain.

\paragraph{2.} Some math notations:
\begin{enumerate}
    \item $r$ is the ground-truth.
    \item $\hat{r} is the proposed response.$
    \item $w_j$ is the $j$'th token in $r$.
    \item $\hat{r}_j$ is the $j$'th token in $\hat{r}$.
\end{enumerate}

\paragraph{BLEU.}
\begin{enumerate}
    \item Author: Papineni et al., 2002a.
    \item Details on the computation with math formulas.
    \item Sentence-level smoothing (Chen and Cherry 2014.)
    \item Most commonly $N = 4$.
    \item Usually compute on corpus-level.
    \item Originally designed to use multiple references.
\end{enumerate}

\paragraph{METEOR.}
\begin{enumerate}
    \item Author: Banjerjee and Lavie, 2005.
    \item Address several weakness of BLEU.
    \item Computation process is breifly described (No formula).
    \item Multiple modules for alignment computation.
    \item Harmonic mean of precision and recall using the alignment.
\end{enumerate}

\paragraph{ROUGE.}
\begin{enumerate}
    \item Only ROUGE-L is considered.
    \item F-measure based on LCS.
    \item LCS is breifly discussed (No formula).
\end{enumerate}

\subsection{Embedding-based Metrics}
\paragraph{1.} Explain briefly how this kind of metrics work:
\begin{enumerate}
    \item Assign a vector to each word using methods such Word2Vec.
    \item Combine vectors of individual words to get sentence-level embeddings
    using some heuristic.
    \item Compare sentence embeddings using measure like cosine distance.
\end{enumerate}

\paragraph{Greedy Matching.}
Detail introduction with formula:
\begin{enumerate}
    \item Does not compute sentence-level embeddings.
    \item Originally introduced for intelligent tutoring systems.
    \item Favours responses with key words semantically similar to those in the
    ground truth.
\end{enumerate}

\paragraph{Embedding Average.}
Introuduce with references and formulas:
\begin{enumerate}
    \item Calculate sentence embeddings with additive composition.
    \item Widely used in domains such as textual similarity tasks.
    \item Refs: Foltz et al.,
1998; Landauer and Dumais, 1997; Mitchell and
Lapata, 2008.
\end{enumerate}

\paragraph{Vector Extrema.}
\begin{enumerate}
    \item Forgues et al., 2014.
    \item Take the most extreme value amongst all word vectors in the sentence
    and use that value in the sentence's embedding.
    \item Prioritizes informative words over common ones.
    \item More likely to ignore common words.
\end{enumerate}

\section{Dialogue Response Generation Models}
\paragraph{1.}
Obtain responses from a diverse range of response generation models.

\subsection{Retrieval Models}
\paragraph{1.}
Retrieval models are typically evaluated based on whether they can retrieve
the ground-truth given a corpus of predefined responses, with recall or precision.
But we remove one occurrence of the ground-truth from the corpus since this minics
the real deployment setting more closely.

\paragraph{2.}
We evaluate these models by comparing its responses to the ground-truth since
this imitates real-life deployment of these models.
Note these retrieval models all come from
(Lowe et al. 2015, the Ubuntu Dialogue Corpus)

\paragraph{TF-IDF}
\begin{enumerate}
    \item Capture how importance a given word is to some document.
    \item There are two variants: C-TFIDF uses similarity between input context
    and corpus contexts while R-TFIDF uses similarity between input contexts and
    corpus responses.
\end{enumerate}

\paragraph{Dual Encoder (DE)}
\begin{enumerate}
    \item Two RNN to encode the context $c$ and the response $\hat{r}$ respectively.
    \item Caculate the the probability of $\hat{r}$ being the ground-truth with:
    \begin{align}
        p(\hat{r} = r) = \sigma(c^T M \hat{h} + b)
    \end{align}
    $M, b$ are learned parameters.
    \item Training is done with negative sampling to minimize cross-entropy
    of all $(c, r)$ pairs.
\end{enumerate}

\subsection{Generative Models}
\paragraph{1.}
A model is generative if it can generate entirely new sentences unseen in the
training set.

\paragraph{LSTM language model.}
\begin{enumerate}
    \item The baseline model.
    \item Trained to predict the next word in the $(c, r)$ pair.
    \item Use a greedy beam search at test time.
\end{enumerate}

\paragraph{HRED}
\begin{enumerate}
    \item Traditional Encoder-Decoder concatenates all utterances in the context
    all together, which causes recent utterance to outweighed previous ones.
    \item HRED uses a hierarachy of encoders.
    \item Enables the handling to longer-term dependencies.
\end{enumerate}

\subsection{Conclusions from an Incomplte Analysis}

\section{Human Correlation Analysis}
\paragraph{Data Collection.}
\paragraph{Survey Results.}
\paragraph{Qualitative Analysis.}

\section{Discussion}
\paragraph{Constrained tasks.}
\paragraph{Incoporating multiple responses.}
\paragraph{Searching for suitable metrics.}

\end{document}
