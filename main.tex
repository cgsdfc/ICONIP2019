\documentclass[runningheads]{llncs}

\usepackage{graphicx}
\usepackage{array}
\usepackage{booktabs}
\usepackage{float}
\usepackage{hyperref}

\renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
    \title{Yet Another Problem of Automatic Evaluation of Generative Dialogue Systems}

    \titlerunning{Yet Another Problem}

    \author{Cong Feng\inst{1} \and Wenge Rong\inst{1}}

    \authorrunning{C. Feng et al.}

    \institute{
    School of Computer Science and Engineering, Beihang University, Beijing, China
    \email{cgsdfc@126.com} \\
    \email{w.rong@buaa.edu.cn}
    }

    \maketitle

    \begin{abstract}
        Evaluating generative dialogue systems with automatic metrics is a difficult problem. It has been shown that metrics based on word overlap and word embedding correlate poorly with human judgment. In this paper, we highlight yet another problem of evaluation done with metrics through an empirical study. We show that regardless of the correlation with human judgment, the scores of the metrics do not correlate well with one another, which means fine-tuning the system on one set of metrics may yield a gotcha when testing with a different set of metrics. This infeasibility again adds to the ineffectiveness of evaluation done alone with metrics. To the end of utilizing metrics at its best, we offer several pragmatic pieces of advice on the use of automatic metrics to avoid the issue of metric inconsistency.
        \keywords{Automatic Metrics, Response Generation, Chatbot}
    \end{abstract}


    \section{Introduction}
    The chat-oriented dialogue systems have seen a boom in the recent literature of dialogue systems. These systems are trained to make an appropriate response given a conversational context and can be applied to various chatbot applications, language education tools, and intelligent personal assistants. This setting is different from that of the traditional task-oriented dialogue systems, which are programmed to assist their users on completion of a specific task, such as restaurant reservation and airline information acquisition. As such, the chat-oriented dialogue systems cannot be evaluated in the same supervised setting as their task-oriented counterpart, which turns out to bring a greater challenge.

    As an advanced technique to implement the chat-oriented system, the generative model based on neural network architecture feature end-to-end training with minimal hard-coded features and fluent human-like output sentences. Given a large corpus of context-response pairs, these neural models can learn language patterns and extract common-sense knowledge from the corpus in an unsupervised manner, making it a highly plausible solution to the response generation problem. The research community has made steady progress with the generative models. Vinyals et al. \cite{GoogleChatbot} apply the Seq2Seq model \cite{Seq2Seq} to generate responses to both technical queries in an IT helpdesk and open-ended questions such as \textit{what's the purpose of life?} and their model consistently beats a rule-based chatbot. However, generative models are not without problems. They inclined to give a meaningless response to the questions they are incapable to handle, such as \textit{I don't know what you're talking about} and like any chat-oriented systems, their evaluation remains an open problem.

    Previous works of generative dialogue systems begin by borrowing automatic metrics from the machine translation community, such as the popular BLEU \cite{BLEU} and METEOR \cite{METEOR}. However the correlation between these borrowed metrics and human judgment remains unclear and researchers generally turn to human evaluation for better accuracy and reliability \cite{Shang,DCGM,VHRED}. Liu et al. \cite{HowNot} perform an empirical study on how well these automatic metrics correlate with human judgment. They conclude that the metrics only correlate weakly with human judgment on the non-technical Twitter corpus and not at all on the technical Ubuntu Dialogue Corpus \cite{ubuntu_corpus}. The conclusion urges against the use of these metrics and calls for the development of new automatic metrics that are more relevant to human judgment.

    In this paper, we extend the work of Liu et al. and investigate the behaviors of automatic metrics when human judgment is not available. In particular, we highlight the drawbacks of existing metrics by trying to compare system performance purely based on them and show how inconsistent the results can be. Besides, we draw samples from the per-utterance scores of different metrics and analyze the pairwise correlation of these samples. We find that some pairs of samples have a high correlation, while other pairs show a much lower one. Following this observation, we cluster the metrics based on the correlation of their corresponding samples. The results show that metrics based on the same set of features tend to cluster together, indicating a high pairwise correlation within the same group. Our experimental results show yet another pitfall of using automatic metrics that the results from different metrics have the risk of high inconsistency if they don't happen to be in the same group. We therefore strongly advise researchers to wisely choose their metric to use to promote the comparison across different models and datasets.

    \section{Related Work}
    Previous works on automatic metrics have mostly focused on their correlation with human judgment. After all, the advantages of automatic metrics would be largely compromised if they cannot reflect human judgment. In the field of machine translation, where metrics have been established to reduce human labor, almost every newly proposed metrics report an improved human correlation, as in \cite{NIST}, \cite{METEOR}, and \cite{chrf}. In the field of generative dialogue system, to address the issue of low human correlation found by \cite{HowNot}, various semantic-based metrics have been proposed and achieve improvement to different extends, such as the ADEM metric \cite{ADEM} proposed by Lowe et al. and the RUBER metric \cite{RUBER} proposed by Tao et al. In other words, correlation with human judgment has always been the supervise signals guiding the evolution of automatic metrics.

    On the other hand, automatic metrics have constantly been doubted about their capability to reflect human judgment. In one of the early attempts to the response generation problem, Ritter et al. make an initial examination on the suitability of BLEU to this field leveraging the human data they collect \cite{Ritter11}. They found the BLEU scores were very low even on the system level and the correlation is modest. Another popular metric is perplexity, which is the de facto standard to evaluate statistical language models. Vinyals et al. show that their Seq2Seq-based model has a much lower perplexity than the n-gram based model, but they also admit the drawbacks of using metrics \cite{GoogleChatbot}. Serban et al. also use perplexity to evaluate their models in \cite{HRED}, along with other metrics. Like Vinyals et al, they are unclear how well these metrics account for the grammatical correctness and semantic coherence of the models.

    \section{Experiment Setup}
    Explain the metrics, models and datasets we used.

    \subsection{Metrics}
    Introduce metrics, one per paragraph.
    \paragraph{BLEU.}
    \paragraph{METEOR.}
    \paragraph{ROUGE.}

    \paragraph{Greedy Matching.}
    \paragraph{Embedding Average.}
    \paragraph{Vector Extrema.}

    \paragraph{Distinct-N.}
    \paragraph{Utterance length.}

    \subsection{Models}
    Introduce models, one per paragraph.
    \paragraph{LSTM language model.}
    \paragraph{HRED.}
    \paragraph{VHRED.}

    \subsection{Datasets}
    Introduce datasets, one per paragraph.
    \paragraph{Ubuntu Dialogue Corpus}
    \paragraph{OpenSubtitles}
    \paragraph{LSDSCC}


    \section{Experimental Study}
    \paragraph{Statistical Analysis.}
    Display the clustering of metrics through multiple perspectives,
    such as the similarity of their distribution plots, tables of Pearson and Spearman Correlation
    and the visualization outcomes of a certain clustering algorithm.

    \paragraph{Qualitative Analysis.}
    Display and analyze some examples where different metrics disagree greatly.
    For example, a response with no n-gram overlap will get zero BLEU but if it is semantically
    relevant to the message or reference, embedding-based metric will give it a high score.

    \section{Discussion}
    Conclusions from the experiment.
    Advice on using metrics.

    \subsection*{Acknowledgements.}

    \bibliographystyle{splncs04}
    \bibliography{data/all}

\end{document}
